# This is an example of a workflow producing an S3 output artifact which is saved to a hard-wired
# location. This is useful for workflows which want to publish results to a well known or
# pre-determined location.
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: output-artifact-s3-
spec: 
  entrypoint: output-artifact-s3-example
  templates:
  - name: output-artifact-s3-example
    container:
      image: docker/whalesay:latest
      command: [sh, -c]
      args: ["echo Hello, this is a S3 demo file | tee /tmp/s3_demo.txt"]
    outputs:
      artifacts:
      - name: message
        path: /tmp/s3_demo.txt
        # It is possible to disable tar.gz archiving by setting the archive strategy to 'none'
        # Disabling archiving has the following limitations on S3: symbolic links will not be
        # uploaded, as S3 does not support the concept/file mode of symlinks.
        archive:
           none: {}

        s3:
          # Use the corresponding endpoint depending on your S3 provider:
          #   AWS: s3.amazonaws.com
          #   GCS: storage.googleapis.com
          #   Minio: my-minio-endpoint.default:9000
          endpoint: s3.amazonaws.com
          bucket: test-evo
          # Specify the bucket region. Note that if you want Argo to figure out this automatically,
          # you can set additional statement policy that allows `s3:GetBucketLocation` action.
          # For details, check out: https://argoproj.github.io/argo-workflows/configure-artifact-repository/#configuring-aws-s3
          region: us-east-1

          # NOTE: by default, output artifacts are automatically tarred and gzipped before saving.
          # As a best practice, .tgz or .tar.gz should be suffixed into the key name so the
          # resulting object has an accurate file extension and mime-type. If archive is set to
          # 'none', then preserve the appropriate file extension for the key name
          key: output/s3_demo.txt

          # accessKeySecret and secretKeySecret are secret selectors. It references the k8s secret
          # named 'my-s3-credentials'. This secret is expected to have have the keys 'accessKey'
          # and 'secretKey', containing the base64 encoded credentials to the bucket.
          accessKeySecret:
            name: my-s3-credentials
            key: accessKey
          secretKeySecret:
            name: my-s3-credentials
            key: secretKey